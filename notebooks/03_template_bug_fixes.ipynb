{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a94ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rdflib pyparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a767cb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from rdflib.plugins.sparql.parser import parseQuery\n",
    "import mwapi\n",
    "from mwapi.errors import APIError\n",
    "import mwparserfromhell as parser\n",
    "import re\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# constants\n",
    "templates = [\n",
    "    \"Wikidata list\",\n",
    "    \"SPARQL\",\n",
    "    \"SPARQL2\",\n",
    "    \"SPARQL5\",\n",
    "    \"SPARQL Inline\",\n",
    "    \"Wdquery\",\n",
    "    \"Complex constraint\"\n",
    "]\n",
    "\n",
    "template_regex_string = \"|\".join([f\"{{{{\\s*[{t[0].lower()}|{t[0].upper()}]{t[1:]}\\s*\\|\" for t in templates])\n",
    "\n",
    "wikis = set()\n",
    "\n",
    "with open('wikis.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        wikis.add(f'https://{line[:-1]}')\n",
    "\n",
    "big_wikis = [\n",
    "    'https://en.wikipedia.org',\n",
    "    'https://fr.wikipedia.org',\n",
    "    'https://de.wikipedia.org',\n",
    "    'https://ja.wikipedia.org',\n",
    "    'https://ru.wikipedia.org',\n",
    "    'https://pt.wikipedia.org',\n",
    "    'https://it.wikipedia.org',\n",
    "    'https://zh.wikipedia.org',\n",
    "    'https://fa.wikipedia.org',\n",
    "    'https://ar.wikipedia.org',\n",
    "    'https://commons.wikimedia.org',\n",
    "    'https://wikidata.org',\n",
    "    'https://mediawiki.org'\n",
    "]\n",
    "\n",
    "wikis.update(big_wikis)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(filename='example.log', encoding='utf-8', level=logging.DEBUG)\n",
    "\n",
    "# helper functions\n",
    "def is_sparql_query_valid(query):\n",
    "    try:\n",
    "        # Attempt to prepare a SPARQL query. This will parse the query.\n",
    "        parseQuery(query)\n",
    "        return True  # If parsing succeeds, the query is valid.\n",
    "    except:\n",
    "        return False  # If parsing fails, the query is invalid.\n",
    "    \n",
    "def get_transcluded_pages(session, template):\n",
    "    continued = session.get(\n",
    "        formatversion=2,\n",
    "        action='query',\n",
    "        prop='transcludedin',\n",
    "        titles=f\"Template:{template}\",\n",
    "        continuation=True\n",
    "    )\n",
    "\n",
    "    pages = []\n",
    "    try:\n",
    "        for portion in continued:\n",
    "            if 'query' in portion:\n",
    "                for page in portion['query']['pages']:\n",
    "                    try:\n",
    "                        for transcluded in page['transcludedin']:\n",
    "                            pages.append(transcluded[\"title\"])\n",
    "                    except:\n",
    "                        pass\n",
    "            else:\n",
    "                logger.error(\"MediaWiki returned empty result batch.\")\n",
    "    except APIError as error:\n",
    "        raise ValueError(\n",
    "            \"MediaWiki returned an error:\", str(error)\n",
    "        )\n",
    "    \n",
    "    return pages\n",
    "\n",
    "def extract_sparql(session, p, t):\n",
    "    resp = session.get(\n",
    "        formatversion=2,\n",
    "        action='query',\n",
    "        prop='revisions',\n",
    "        rvslots='*',\n",
    "        rvprop='content',\n",
    "        titles=p\n",
    "    )\n",
    "\n",
    "    content = resp['query']['pages'][0]['revisions'][0]['slots']['main']['content']\n",
    "    wikitext = parser.parse(content)\n",
    "    templates = wikitext.filter_templates()\n",
    "    templates = list(filter(lambda template: t in template, templates))\n",
    "    if t == \"Wikidata list\":\n",
    "        templates = list(filter(lambda template: template != \"{{Wikidata list end}}\", templates))\n",
    "    \n",
    "    out = []\n",
    "    for template in templates:\n",
    "        out.append(template.split(\"|\")[1].split(\"=\")[1])\n",
    "        \n",
    "    return out\n",
    "\n",
    "def check_templates(template):\n",
    "    for t in templates:\n",
    "        if t in template:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def split_string_and_extract_preceding(s, delimiter):\n",
    "    parts = s.split(delimiter)  # Split the string by the delimiter.\n",
    "    preceding_texts = []  # Initialize a list to hold the preceding text segments.\n",
    "    \n",
    "    search_pos = 0  # Start position for each search iteration.\n",
    "    for part in parts[:-1]:  # Ignore the last part since no split occurs after it.\n",
    "        # Calculate the start position of the current part in the original string.\n",
    "        current_part_start = s.find(part, search_pos)\n",
    "        # Calculate the end position of the current part, which is the split point.\n",
    "        split_point = current_part_start + len(part)\n",
    "        \n",
    "        # Determine the start position for extracting preceding characters.\n",
    "        # It's the greater of 0 and split_point - 300 to avoid negative indices.\n",
    "        extract_start = max(0, split_point - 300)\n",
    "        \n",
    "        # Extract up to 250 characters preceding the split point.\n",
    "        preceding_text = s[extract_start:split_point]\n",
    "        preceding_texts.append(preceding_text)\n",
    "        \n",
    "        # Update the search position for the next iteration.\n",
    "        search_pos = split_point + len(delimiter)\n",
    "    \n",
    "    return preceding_texts[0]\n",
    "\n",
    "def get_sparql_and_surrounding(title):\n",
    "    out = []\n",
    "    resp = session.get(\n",
    "        formatversion=2,\n",
    "        action='query',\n",
    "        prop='revisions',\n",
    "        rvslots='*',\n",
    "        rvprop='content',\n",
    "        titles=title\n",
    "    )\n",
    "    content = resp['query']['pages'][0]['revisions'][0]['slots']['main']['content']\n",
    "    wikitext = parser.parse(content)\n",
    "    wikitext_templates = list(filter(check_templates, wikitext.filter_templates()))\n",
    "    wikitext_templates = list(filter(lambda template: template != \"{{Wikidata list end}}\", wikitext_templates))\n",
    "    wikitext_templates = list(filter(lambda template: template != \"{{Wikidata list header}}\", wikitext_templates))\n",
    "    wikitext_templates = list(filter(lambda template: template != \"{{Wikidata list menu}}\", wikitext_templates))\n",
    "    wikitext_templates = list(filter(lambda template: template != \"{{Wikidata list documentation}}\", wikitext_templates))\n",
    "    if '{{query page' in wikitext:\n",
    "        lede = wikitext[:250]\n",
    "        query = re.split(\"query\\s*=\\s*\", str(wikitext))[1].split(\"|\")[0]\n",
    "        text = None\n",
    "        results = None\n",
    "        if not is_sparql_query_valid(query):\n",
    "            logger.info(f'invalid query: {query}')\n",
    "\n",
    "        out.append({\"title\": title, \"lede\": lede, 'preceding_text': text, 'query': query, 'results': results})\n",
    "    \n",
    "    elif len(wikitext_templates) > 0:\n",
    "        for wt in wikitext_templates:\n",
    "            lede = wikitext[:250]\n",
    "            text = split_string_and_extract_preceding(wikitext, str(wt))\n",
    "            results = None\n",
    "            if \"wdquery\" in wt.lower():\n",
    "                query = re.split(\"query\\s*=\\s*\", str(wt))[1].split(\"|\")[0]\n",
    "            elif \"complex constraint\" in wt.lower():\n",
    "                lede = re.split(\"label\\s*=\\s*\", str(wt))[1].split(\"|\")[0]\n",
    "                text = re.split(\"description\\s*=\\s*\", str(wt))[1].split(\"|\")[0]\n",
    "                query = re.split(\"sparql\\s*=\\s*\", str(wt))[1].split(\"|\")[0]\n",
    "            elif \"wikidata list\" in wt.lower():\n",
    "                ts = wikitext.find(str(wt))\n",
    "                te = wikitext.lower().find(\"{{wikidata list end}}\")\n",
    "                truncated = wikitext[ts:te]\n",
    "                results = truncated[truncated.find(\"{|\"):truncated.find(\"|}\")]\n",
    "                query = re.split(\"sparql\\s*=\\s*\", str(wt), maxsplit=1)[1].split(\"|\")[0]\n",
    "                if not is_sparql_query_valid(query):\n",
    "                    query = re.split(\"sparql\\s*=\\s*\", str(wt), maxsplit=1)[1].split(\"\\n|\")[0]\n",
    "            elif \"doc example\" in wt.lower():\n",
    "                query = re.split(\"content=\\s*<pre>\\s*{{SPARQL\\s*|\\s*query=\", str(wt))[1]\n",
    "            elif \"sparql label\" in wt.lower():\n",
    "                continue\n",
    "            else:\n",
    "                query = wt.split(\"|\")[1].split(\"=\", 1)[1]\n",
    "                if not is_sparql_query_valid(query):\n",
    "                    query = re.split(\"query\\s*=\\s*\", str(wt), maxsplit=1)[1]\n",
    "                \n",
    "            \n",
    "            if query.endswith(\"\\n}}\"):\n",
    "                query = query[:-3]\n",
    "            if query.endswith(\"}}\"):\n",
    "                query = query[:-2]\n",
    "            query = query.replace(\"{{!}}\", \"|\")\n",
    "            if not is_sparql_query_valid(query):\n",
    "                logger.info(f'invalid query: {query}')\n",
    "\n",
    "            out.append({\"title\": title, \"lede\": lede, 'preceding_text': text, 'query': query, 'results': results})\n",
    "        return out\n",
    "    return None\n",
    "\n",
    "# main function\n",
    "def main():\n",
    "    df = pd.DataFrame(columns=['project', 'title', 'lede', 'preceding_text', 'query', 'results'])\n",
    "    for w in wikis:\n",
    "        fail_ctr = 0\n",
    "        logger.info(w)\n",
    "        session = mwapi.Session(w, user_agent=\"htriedman sparql corpus bot\")\n",
    "        all_pages = set()\n",
    "        for t in templates:\n",
    "            pages = get_transcluded_pages(session, t)\n",
    "            logger.info(f'template {t} occurs {len(pages)} times on {w}')\n",
    "            all_pages.update(pages)\n",
    "        logger.info(f'there are a total of {len(all_pages)} sparql-related pages on {w}')  \n",
    "        for i, p in enumerate(all_pages):\n",
    "            if i % 500 == 0:\n",
    "                logger.info(f'templates seen: {i}')\n",
    "            try:\n",
    "                out = get_sparql_and_surrounding(p)\n",
    "                if out is None:\n",
    "                    continue\n",
    "                for i in out:\n",
    "                    out[i]['project'] = w\n",
    "                df = pd.concat([df, pd.DataFrame.from_dict(out)])\n",
    "            except:\n",
    "                fail_ctr += 1\n",
    "                if fail_ctr % 50 == 0 and fail_ctr != 0:\n",
    "                    logger.info(f'failures: {fail_ctr}')\n",
    "                continue\n",
    "\n",
    "    df['validity'] = df['query'].map(is_sparql_query_valid)\n",
    "    df.to_pickle('wikidata-sparql-templates-bug-fixes.pkl')\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401c119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib.plugins.sparql.parser import parseQuery\n",
    "import mwapi\n",
    "from mwapi.errors import APIError\n",
    "import mwparserfromhell as parser\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dcca23",
   "metadata": {},
   "source": [
    "## Check on functionality of queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3941a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_sparql_query_valid(query):\n",
    "    try:\n",
    "        # Attempt to prepare a SPARQL query. This will parse the query.\n",
    "        parseQuery(query)\n",
    "        return True  # If parsing succeeds, the query is valid.\n",
    "    except:\n",
    "        return False  # If parsing fails, the query is invalid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e329ce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('wikidata-sparql-templates.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f118fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:2000].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea59e106",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['validity'] = df['query'].map(is_sparql_query_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da33b83c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85d7818",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_transcluded_pages(session, template):\n",
    "    continued = session.get(\n",
    "        formatversion=2,\n",
    "        action='query',\n",
    "        prop='transcludedin',\n",
    "        titles=f\"Template:{template}\",\n",
    "        continuation=True\n",
    "    )\n",
    "\n",
    "    pages = []\n",
    "    try:\n",
    "        for portion in continued:\n",
    "            if 'query' in portion:\n",
    "                for page in portion['query']['pages']:\n",
    "                    try:\n",
    "                        for transcluded in page['transcludedin']:\n",
    "                            pages.append(transcluded[\"title\"])\n",
    "                    except:\n",
    "                        pass\n",
    "            else:\n",
    "                print(\"MediaWiki returned empty result batch.\")\n",
    "    except APIError as error:\n",
    "        raise ValueError(\n",
    "            \"MediaWiki returned an error:\", str(error)\n",
    "        )\n",
    "    \n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2e09c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sparql(session, p, t):\n",
    "    resp = session.get(\n",
    "        formatversion=2,\n",
    "        action='query',\n",
    "        prop='revisions',\n",
    "        rvslots='*',\n",
    "        rvprop='content',\n",
    "        titles=p\n",
    "    )\n",
    "\n",
    "    content = resp['query']['pages'][0]['revisions'][0]['slots']['main']['content']\n",
    "    wikitext = parser.parse(content)\n",
    "    templates = wikitext.filter_templates()\n",
    "    templates = list(filter(lambda template: t in template, templates))\n",
    "    if t == \"Wikidata list\":\n",
    "        templates = list(filter(lambda template: template != \"{{Wikidata list end}}\", templates))\n",
    "    \n",
    "    out = []\n",
    "    for template in templates:\n",
    "        out.append(template.split(\"|\")[1].split(\"=\")[1])\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa90cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_templates(template):\n",
    "    for t in templates:\n",
    "        if t in template:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def split_string_and_extract_preceding(s, delimiter):\n",
    "    parts = s.split(delimiter)  # Split the string by the delimiter.\n",
    "    preceding_texts = []  # Initialize a list to hold the preceding text segments.\n",
    "    \n",
    "    search_pos = 0  # Start position for each search iteration.\n",
    "    for part in parts[:-1]:  # Ignore the last part since no split occurs after it.\n",
    "        # Calculate the start position of the current part in the original string.\n",
    "        current_part_start = s.find(part, search_pos)\n",
    "        # Calculate the end position of the current part, which is the split point.\n",
    "        split_point = current_part_start + len(part)\n",
    "        \n",
    "        # Determine the start position for extracting preceding characters.\n",
    "        # It's the greater of 0 and split_point - 300 to avoid negative indices.\n",
    "        extract_start = max(0, split_point - 300)\n",
    "        \n",
    "        # Extract up to 250 characters preceding the split point.\n",
    "        preceding_text = s[extract_start:split_point]\n",
    "        preceding_texts.append(preceding_text)\n",
    "        \n",
    "        # Update the search position for the next iteration.\n",
    "        search_pos = split_point + len(delimiter)\n",
    "    \n",
    "    return preceding_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192ea88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparql_and_surrounding(title):\n",
    "    out = []\n",
    "    resp = session.get(\n",
    "        formatversion=2,\n",
    "        action='query',\n",
    "        prop='revisions',\n",
    "        rvslots='*',\n",
    "        rvprop='content',\n",
    "        titles=title\n",
    "    )\n",
    "    content = resp['query']['pages'][0]['revisions'][0]['slots']['main']['content']\n",
    "    wikitext = parser.parse(content)\n",
    "    wikitext_templates = list(filter(check_templates, wikitext.filter_templates()))\n",
    "    wikitext_templates = list(filter(lambda template: template != \"{{Wikidata list end}}\", wikitext_templates))\n",
    "    wikitext_templates = list(filter(lambda template: template != \"{{Wikidata list header}}\", wikitext_templates))\n",
    "    wikitext_templates = list(filter(lambda template: template != \"{{Wikidata list menu}}\", wikitext_templates))\n",
    "    wikitext_templates = list(filter(lambda template: template != \"{{Wikidata list documentation}}\", wikitext_templates))\n",
    "    if '{{query page' in wikitext:\n",
    "        print(\"query page\")\n",
    "        lede = wikitext[:250]\n",
    "        query = re.split(\"query\\s*=\\s*\", str(wikitext))[1].split(\"|\")[0]\n",
    "        text = None\n",
    "        results = None\n",
    "\n",
    "        out.append({\"title\": title, \"lede\": lede, 'preceding_text': text, 'query': query, 'results': results})\n",
    "    \n",
    "    elif len(wikitext_templates) > 0:\n",
    "        for wt in wikitext_templates:\n",
    "            lede = wikitext[:250]\n",
    "            text = split_string_and_extract_preceding(wikitext, str(wt))\n",
    "            results = None\n",
    "            if \"wdquery\" in wt.lower():\n",
    "#                 print(\"wdquery\")\n",
    "                query = re.split(r\"query\\s*=\\s*\", str(wt))[1].split(\"|\")[0]\n",
    "            elif \"complex constraint\" in wt.lower():\n",
    "#                 print(\"complex constraint\")\n",
    "                lede = re.split(r\"label\\s*=\\s*\", str(wt))[1].split(\"|\")[0]\n",
    "                text = re.split(r\"description\\s*=\\s*\", str(wt))[1].split(\"|\")[0]\n",
    "                query = re.split(r\"sparql\\s*=\\s*\", str(wt))[1].split(\"|\")[0]\n",
    "            elif \"wikidata list\" in wt.lower():\n",
    "#                 print(\"wikidata list\")\n",
    "                ts = wikitext.find(str(wt))\n",
    "                te = wikitext.lower().find(\"{{wikidata list end}}\")\n",
    "                truncated = wikitext[ts:te]\n",
    "                results = truncated[truncated.find(\"{|\"):truncated.find(\"|}\")]\n",
    "#                 print(re.split(\"sparql\\s*=\\s*\", str(wt)))\n",
    "                valid = False\n",
    "                i = 0\n",
    "                possible_splits = [r\"\\|section\", r\"\\|\", r\"\\s+\\|\"]\n",
    "                while not valid:\n",
    "                    print(i)\n",
    "                    print(possible_splits[i])\n",
    "                    print(re.split(possible_splits[i], re.split(r\"sparql\\s*=\\s*\", str(wt), maxsplit=1)[1]))\n",
    "                    query = re.split(possible_splits[i], re.split(r\"sparql\\s*=\\s*\", str(wt), maxsplit=1)[1])[0]#.split(\"|\")[0]\n",
    "                    valid = is_sparql_query_valid(query)\n",
    "                    i += 1\n",
    "                    if i >= len(possible_splits):\n",
    "                        break\n",
    "#                 query = re.split(r\"sparql\\s*=\\s*\", str(wt), maxsplit=1)[1].split(\"|\")[0]\n",
    "#                 if not is_sparql_query_valid(query):\n",
    "#                     query = re.split(\"sparql\\s*=\\s*\", str(wt), maxsplit=1)[1].split(\"|section\")[0]\n",
    "#                     if not is_sparql_query_valid(query):\n",
    "#                         query = re.split(\"sparql\\s*=\\s*\", str(wt), maxsplit=1)[1].split(\"\\s+|\")[0]\n",
    "            elif \"doc example\" in wt.lower():\n",
    "                query = re.split(r\"content=\\s*<pre>\\s*{{SPARQL\\s*|\\s*query=\", str(wt))[1]\n",
    "            elif \"sparql label\" in wt.lower():\n",
    "                continue\n",
    "            else:\n",
    "#                 print(\"other (SPARQL, SPARQL2, SPARQL5, SPARQL Inline)\")\n",
    "#                 print(f'{wt.split(\"|\")}')\n",
    "                query = wt.split(\"|\")[1].split(\"=\", 1)[1]\n",
    "                if not is_sparql_query_valid(query):\n",
    "                    query = re.split(r\"query\\s*=\\s*\", str(wt), maxsplit=1)[1]\n",
    "                \n",
    "            \n",
    "            if query.endswith(\"\\n}}\"):\n",
    "                query = query[:-3]\n",
    "            if query.endswith(\"}}\"):\n",
    "                query = query[:-2]\n",
    "            query = query.replace(\"{{!}}\", \"|\")\n",
    "            if not is_sparql_query_valid(query):\n",
    "                print(f'invalid query: {query}')\n",
    "\n",
    "            out.append({\"title\": title, \"lede\": lede, 'preceding_text': text, 'query': query, 'results': results})\n",
    "        return out\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba772c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = [\n",
    "    \"Wikidata list\",\n",
    "    \"SPARQL\",\n",
    "    \"SPARQL2\",\n",
    "    \"SPARQL5\",\n",
    "    \"SPARQL Inline\",\n",
    "    \"Wdquery\",\n",
    "    \"Complex constraint\"\n",
    "]\n",
    "\n",
    "template_regex_string = \"|\".join([f\"{{{{\\s*[{t[0].lower()}|{t[0].upper()}]{t[1:]}\\s*\\|\" for t in templates])\n",
    "\n",
    "wikis = set()\n",
    "\n",
    "with open('wikis.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        wikis.add(f'https://{line[:-1]}')\n",
    "\n",
    "big_wikis = [\n",
    "    'https://en.wikipedia.org',\n",
    "    'https://fr.wikipedia.org',\n",
    "    'https://de.wikipedia.org',\n",
    "    'https://ja.wikipedia.org',\n",
    "    'https://ru.wikipedia.org',\n",
    "    'https://pt.wikipedia.org',\n",
    "    'https://it.wikipedia.org',\n",
    "    'https://zh.wikipedia.org',\n",
    "    'https://fa.wikipedia.org',\n",
    "    'https://ar.wikipedia.org',\n",
    "    'https://commons.wikimedia.org',\n",
    "    'https://wikidata.org',\n",
    "    'https://mediawiki.org'\n",
    "]\n",
    "\n",
    "wikis.update(big_wikis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8001baf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['project', 'title', 'lede', 'preceding_text', 'query', 'results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df9d280",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikis = ['https://ts.wikipedia.org']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f6b937",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for w in wikis:\n",
    "    fail_ctr = 0\n",
    "    print(w)\n",
    "    session = mwapi.Session(w, user_agent=\"htriedman sparql corpus bot\")\n",
    "    all_pages = set()\n",
    "    for t in templates:\n",
    "        pages = get_transcluded_pages(session, t)\n",
    "        print(f'template {t} occurs {len(pages)} times on {w}')\n",
    "        all_pages.update(pages)\n",
    "    print(f'there are a total of {len(all_pages)} sparql-related pages on {w}')  \n",
    "    for i, p in enumerate(all_pages):\n",
    "        if i % 500 == 0:\n",
    "            print(f'templates seen: {i}')\n",
    "#         out = get_sparql_and_surrounding(p)\n",
    "#         if out is None:\n",
    "#             continue\n",
    "#         for o in out:\n",
    "#             o['project'] = w\n",
    "#         df = pd.concat([df, pd.DataFrame.from_dict(out)])\n",
    "        try:\n",
    "            out = get_sparql_and_surrounding(p)\n",
    "            if out is None:\n",
    "                continue\n",
    "            for i in out:\n",
    "                out[i]['project'] = w\n",
    "            df = pd.concat([df, pd.DataFrame.from_dict(out)])\n",
    "        except:\n",
    "#             print(f'failure: {out}')\n",
    "            fail_ctr += 1\n",
    "            if fail_ctr % 50 == 0 and fail_ctr != 0:\n",
    "                print(f'failures: {fail_ctr}')\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c112088",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('SELECT\\n  ?number_of_authors\\n  ?number_of_works\\n  ?item\\n  (REPLACE(STR(?item), \".*Q\", \"Q\") AS ?qid)\\n  (CONCAT( \"[[toolforge:scholia/organization/\", ?qid , \"|\", ENCODE_FOR_URI(?number_of_works), \"]] / \", \"[[toolforge:scholia/organization/\", ?qid , \"/missing|üìñ]]\") AS ?scholia)\\n\\nWITH {  \\n  SELECT\\n    ?item\\n    (COUNT(DISTINCT ?author) AS ?number_of_authors)\\n    (COUNT(DISTINCT ?work) AS ?number_of_works)\\n  WHERE {\\n    ?item wdt:P17 wd:Q1006 .\\n    ?author wdt:P108 {{!}} wdt:P463 {{!}} wdt:P1416/wdt:P361* ?item .\\n    ?work wdt:P50 ?author .\\n  }\\n  GROUP BY ?item \\n} AS %results\\nWHERE {\\n  INCLUDE %results\\n  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en,da,es,fr,jp,nl,no,ru,sv,zh\". }\\n}\\nORDER BY DESC(?number_of_authors) DESC(?number_of_works) ?item ?qid ?scholia\\n\\n|columns=label:Organization,?number_of_authors:Number of authors,?scholia:Publications (known/missing)\\n}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12c673c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pickle5\n",
      "  Downloading pickle5-0.0.12-cp37-cp37m-macosx_10_9_x86_64.whl.metadata (2.0 kB)\n",
      "Downloading pickle5-0.0.12-cp37-cp37m-macosx_10_9_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m125.0/125.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pickle5\n",
      "Successfully installed pickle5-0.0.12\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89480783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fb348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1963d970",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_sparql_query_valid(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e479baa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s.split('\\n\\n|')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b678bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f0a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['validity'] = df['query'].map(is_sparql_query_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd950e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a441b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# failure-prone wikis: commons, cswiki, cawiki, nowiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac7ab33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d7786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('wikidata-sparql-templates-bug-fixes.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d0d1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
